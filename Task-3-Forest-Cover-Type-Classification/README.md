# ğŸŒ² Forest Cover Type Classification

## ğŸ“Œ Project Overview
This project is part of my **Elevvo Pathways Machine Learning Internship (Task 3)**.  
The objective is to classify the type of forest cover based on cartographic and environmental features using **multi-class classification** techniques.

Dataset used: [Covertype (UCI Repository)](https://archive.ics.uci.edu/ml/datasets/covertype)

---

## ğŸ¯ Objectives
- Load, clean, and preprocess the dataset.
- Handle categorical and numerical features appropriately.
- Train and evaluate **Random Forest** and **XGBoost** classifiers.
- Compare model performance with accuracy, confusion matrices, and feature importance.
- Apply **hyperparameter tuning** to optimize both models.

---

## ğŸ› ï¸ Tools & Libraries
- **Python**
- **Pandas** â€“ Data manipulation
- **Matplotlib & Seaborn** â€“ Visualization
- **Scikit-learn** â€“ Random Forest, preprocessing, evaluation
- **XGBoost** â€“ Gradient boosting classifier

---

## ğŸŒ² Dataset

This project uses the Forest CoverType dataset from the UCI ML Repository.

- ğŸ”— [Download Full Dataset](https://archive.ics.uci.edu/ml/machine-learning-databases/covtype/covtype.data.gz)
- ğŸ“„ A sample (`data/sample_covtype.csv`) is included for quick testing.


---

## ğŸ“Š Steps Followed
1. **Load Dataset**
   - UCI Covertype dataset with 581,012 samples and 54 features.
   - Target variable: `Cover_Type` (7 forest cover classes).

2. **Data Exploration**
   - Checked for missing values (none found).
   - Visualized target distribution.
   - Correlation heatmap for numeric features.

3. **Model Training**
   - **Random Forest Classifier** (baseline model).
   - **XGBoost Classifier** (gradient boosting approach).

4. **Model Evaluation**
   - Accuracy scores.
   - Confusion Matrices.
   - Feature Importance analysis.

5. **Hyperparameter Tuning**
   - GridSearchCV for Random Forest.
   - RandomizedSearchCV for XGBoost.

---

## ğŸ“ˆ Results

### âœ… Accuracy (Before Tuning)
- **Random Forest:** 0.9533  
- **XGBoost:** 0.8696  

### âš¡ Best Hyperparameters Found
- **Random Forest:**  
  ```python
  {'max_depth': None, 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 200}
  ```
  Best RF Score (Train CV): **0.9468**

- **XGBoost:**  
  ```python
  {'subsample': 1.0, 'n_estimators': 200, 'max_depth': 10, 'learning_rate': 0.2, 'gamma': 0, 'colsample_bytree': 1.0}
  ```
  Best XGB Score (Train CV): **0.9538**

### ğŸ“Š Final Comparison
| Model            | Accuracy (Before Tuning) | Accuracy (After Tuning) |
|------------------|--------------------------|--------------------------|
| Random Forest    | 0.9533                   | 0.9468 (CV Score)        |
| XGBoost          | 0.8696                   | 0.9538 (CV Score)        |

---

## ğŸ” Key Insights
- **Random Forest** performed better out-of-the-box with very high accuracy.  
- **XGBoost** initially underperformed but achieved competitive performance after hyperparameter tuning.  
- Feature importance analysis highlighted **Elevation, Horizontal Distance to Roadways, and Hillshade features** as strong predictors.  

---

## ğŸ“‚ Project Structure
```
forest-cover-classification/
â”‚â”€â”€ data/
â”‚   â””â”€â”€ sample_covtype.csv
â”‚â”€â”€ visuals/
â”‚   â”œâ”€â”€ confusion_matrix_rf.png
â”‚   â”œâ”€â”€ confusion_matrix_xgb.png
â”‚   â”œâ”€â”€ corr_heatmap.png
â”‚   â”œâ”€â”€ distribution_cover_types.png
â”‚   â”œâ”€â”€ feature_importance_rf.png
â”‚   â”œâ”€â”€ feature_importance_xgb.png
â”‚   â”œâ”€â”€ model_comparison.png
â”‚â”€â”€ forest_cover_classification.ipynb
â”‚â”€â”€ README.md
â”‚â”€â”€ requirements.txt
```

---

## ğŸš€ Next Steps
- Try **LightGBM** or **CatBoost** for further comparison.
- Perform feature engineering to reduce dimensionality.
- Deploy the best model using **Streamlit** for interactive predictions.

---

## ğŸ·ï¸ Tags
#DataScience #MachineLearning #Classification #RandomForest #XGBoost #ElevvoPathways #Internship #Python

---

## ğŸ“œ License
This project is for **educational purposes** as part of the Elevvo Pathways internship.
